{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/pau/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login, login\n",
    "\n",
    "#notebook_login()\n",
    "login('YOUR_HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-3.2-1B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_token = \"...Wait! Let's think step by step...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to add the CoT token into the vocab.\n",
    "if wait_token not in tokenizer.get_vocab():\n",
    "    tokenizer.add_tokens([wait_token])\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "WAIT_TOKEN_ID = tokenizer.encode(wait_token, add_special_tokens=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOT_TOKEN_ID = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-21.304841241849253, 0.002203146594003764)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U = 2e-7\n",
    "MAX_ERROR = np.log(U) - np.log(tokenizer.vocab_size)/2\n",
    "MAX_ERROR, 1/MAX_ERROR**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMSampler:\n",
    "    def __init__(self, model, tokenizer, entropy_threshold=1.0, varentropy_threshold=0.25):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.entropy_threshold = entropy_threshold\n",
    "        self.varentropy_threshold = varentropy_threshold\n",
    "        self.verbose = False\n",
    "        self.use_beam_search = False\n",
    "    \n",
    "    def calculate_entropy(self, logits):\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        valid_probs = probs[(probs > 0) & (torch.log(probs) >= MAX_ERROR)]\n",
    "        valid_probs = valid_probs / torch.sum(valid_probs, dim=-1)\n",
    "        H_X = -torch.sum(valid_probs * torch.log(valid_probs), dim=-1)\n",
    "        return H_X\n",
    "\n",
    "    def calculate_varentropy(self, logits, entropies):\n",
    "        H_X = entropies\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        valid_probs = probs[(probs > 0) & (torch.log(probs) >= MAX_ERROR)]\n",
    "        valid_probs = valid_probs / torch.sum(valid_probs, dim=-1)\n",
    "        log_p = torch.log(valid_probs)\n",
    "        E_log_p_squared = torch.sum(valid_probs * log_p**2, dim=-1)\n",
    "        var_H_X = E_log_p_squared - H_X**2\n",
    "        return var_H_X\n",
    "\n",
    "    def beam_search(self, logits, beam_width, max_beam_steps, temperature):\n",
    "        scores = F.log_softmax(logits / temperature, dim=-1)\n",
    "        beam = [(scores[0, i].item(), [i]) for i in torch.topk(scores[0], beam_width).indices.tolist()]\n",
    "\n",
    "        for _ in range(max_beam_steps - 1):\n",
    "            candidates = []\n",
    "            for score, sequence in beam:\n",
    "                input_sequence = torch.tensor(sequence).unsqueeze(0)\n",
    "                outputs = self.model(input_sequence)\n",
    "                next_logits = outputs.logits[:, -1, :]\n",
    "                next_scores = F.log_softmax(next_logits / temperature, dim=-1)\n",
    "                top_scores, top_indices = next_scores[0].topk(beam_width)\n",
    "                for i, (s, idx) in enumerate(zip(top_scores.tolist(), top_indices.tolist())):\n",
    "                    candidates.append((score + s, sequence + [idx]))\n",
    "            \n",
    "            beam = sorted(candidates, key=lambda x: x[0], reverse=True)[:beam_width]\n",
    "\n",
    "            if all(sequence[-1] == beam[0][1][-1] for _, sequence in beam):\n",
    "                break\n",
    "        return beam[0][1]\n",
    "\n",
    "\n",
    "    def apply_sampling_parameters(self, logits, temperature=1.0, top_k=0, top_p=1.0, min_p=0.0):\n",
    "        logits = logits/temperature\n",
    "\n",
    "        if top_k > 0:\n",
    "            top_k = min(top_k, logits.size(-1))\n",
    "            indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "            logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        if top_p < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
    "            cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(dim=-1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "            logits[indices_to_remove] = float('-inf')\n",
    "\n",
    "        if min_p > 0.0:\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            indices_to_remove = probs < min_p\n",
    "            logits[indices_to_remove] = float('-inf')\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def torch_uniform(self, minval, maxval):\n",
    "        return ((minval - maxval) * torch.rand(1) + maxval).item()\n",
    "\n",
    "    def sample(self, input_text, max_length=100, temperature=1.0, top_k=0, top_p=1.0, min_p=0.0):\n",
    "        input_ids = self.tokenizer.encode(input_text, return_tensors='pt')\n",
    "        output_ids = input_ids.clone()\n",
    "        logits_entropies = []\n",
    "        cot_cooldown = 0\n",
    "        cot_tokens = self.tokenizer.encode(\"... Wait! Let's think step by step:\", add_special_tokens=False)[1:]\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(output_ids)\n",
    "                logits = outputs.logits[:, -1, :]\n",
    "\n",
    "            logits = self.apply_sampling_parameters(logits, temperature, top_k, top_p, min_p)\n",
    "            \n",
    "            if torch.argmax(logits, dim=-1).item() == EOT_TOKEN_ID:\n",
    "                output_ids = torch.cat([output_ids, torch.tensor([[EOT_TOKEN_ID]])], dim=-1)\n",
    "                break\n",
    "\n",
    "            entropies = self.calculate_entropy(logits)\n",
    "            logits_entropies.append(entropies)\n",
    "            varentropy = self.calculate_varentropy(logits, entropies)\n",
    "            high_entropy = entropies.item() > self.entropy_threshold\n",
    "            high_varentropy = varentropy > self.varentropy_threshold\n",
    "            if self.verbose:\n",
    "                print(entropies, entropies.item(), varentropy)\n",
    "                print(high_entropy, high_varentropy)\n",
    "\n",
    "            if not high_entropy and not high_varentropy:\n",
    "                # If the model is very confident, and we don't have many confident options we just take the argmax.\n",
    "                next_token = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
    "\n",
    "            elif high_entropy and not high_varentropy and cot_cooldown <= 0:\n",
    "                # If the model is not very confident, but the uncertainty is spread equally (meaning probs are similar across), we insert a CoT token.\n",
    "                next_token = torch.tensor([cot_tokens])\n",
    "                cot_cooldown = 100\n",
    "\n",
    "            elif (high_entropy and high_varentropy) or (high_entropy and not high_varentropy and cot_cooldown > 0):\n",
    "                # The model is not confident and the uncertainty is high, meaning multiple options are possible/acceptable.\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1).t()\n",
    "\n",
    "            else:\n",
    "                # The model is confident, but still uncertain. Beam search looks at possible futures and returns the token that results in the likeliest option.\n",
    "                # If you are gpu poor this can be heavy.\n",
    "                if self.verbose:\n",
    "                    print(\" | branching | \", end='')\n",
    "                if self.use_beam_search:\n",
    "                    next_token = torch.tensor([[self.beam_search(logits, 50, 3, temperature * (1 + self.torch_uniform(-0.1, 0.1)))[0]]])\n",
    "                else:\n",
    "                    probs = torch.softmax(logits, dim=-1)\n",
    "                    next_token = torch.multinomial(probs, num_samples=1).t()\n",
    "            \n",
    "            output_ids = torch.cat([output_ids, next_token], dim=-1)\n",
    "            cot_cooldown -= 1\n",
    "            if next_token.size(1) == 1 and next_token.item() == EOT_TOKEN_ID:\n",
    "                break\n",
    "            print(self.tokenizer.decode(next_token[0]), end='')\n",
    "\n",
    "        return self.tokenizer.decode(output_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = LLMSampler(model, tokenizer, 1, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = sampler.sample(\"Which number is larger: 9.9 or 9.11?\", temperature=1.1, top_k=3000, top_p=0.8, min_p=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|>Which number is larger: 9.9 or 9.11? Are they the same Wait! Let's think step by step: 9.9 = 9 + 0.9 = 9.1 9.11 = 9 + 0.11 = 9.11 The numbers are the same.<|end_of_text|>\""
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|>Which number is larger: 9.9 or 9.11? Wait! Let's think step by step: 9.9 = 9 + 0.9 9.11 = 9 + 0.11 9.9 is larger because 0.9 > 0.11.<|end_of_text|>\""
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|>Which number is larger: 9.9 or 9.11? Wait! Let's think step by step: First, 9.9 is larger than 9.11 because 9.9 has the greater value of the two numbers.<|end_of_text|>\""
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e4b47394c27fd1b5b961f16d7baa9b3fd56cb1998cc7b83060d2a5ccace4e28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
